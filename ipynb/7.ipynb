{"cells":[{"cell_type":"markdown","source":"## Рабочая тетрадь No7","metadata":{"cell_id":"b282a6a3c8674da6854d06e6a85404cc","formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"### 1.1.1 Пример\nРассмотрим программу обучения персептрона на языке Python. Сначала рассмотрим основной класс персептрона, который умеет учиться по тестовым данным.","metadata":{"cell_id":"afe71d6c9bde456a9516e62bb46f69f5","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# класс, который реализует перспептрон и его обучение\nclass Perceptron:\n    def __init__(self,N):\n        # создать нулевые веса\n        self.w = list()\n        for i in range(N):\n            self.w.append( 0)\n    #метод для вычисления значения перспептрона\n    def calc(self,x):\n        res = 0\n        for i in range(len(self.w)):\n            res = res + self.w[i] * x[i]\n        return res\n    # пороговая функция активации перспептрона\n    def sign(self,x):\n        if self.calc(x) > 0:\n            return 1\n        else:\n            return -1\n    # обучение на одном примере\n    def learn(self, la, x, y):\n        #обучаем только, когда результат неверный\n        if y * self.calc(x) <= 0:\n            for i in range(len(self.w)):\n                self.w[i] = self.w[i] + la * y * x[i]\n    #обучение по всем данным Т - кортеж примеров\n    def learning(self, la, T):\n        #цикл обучения\n        for n in range(100):\n            #обучение по всем набору примеров\n            for t in T:\n                self.learn(la,t[0], t[1])","metadata":{"cell_id":"7f31dcdd4eb04a31b8b811cb2dab67f4","source_hash":"79a5c1a8","execution_start":1686039495396,"execution_millis":9,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"В строке 25 мы осуществляем корректировку весов. Посмотрим, как учится\nи работает наш персептрон.","metadata":{"cell_id":"a02fc7121e8444bea34f593672485b70","formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# создаем класс двумерного перспетрона\nperceptron = Perceptron(2)\nla = 0.1 # константа обучения\n# создаём примеры\nT = list()\nT.append([[2,1],1])\nT.append([[3,2],1])\nT.append([[4,1],1])\nT.append([[1,2],-1])\nT.append([[2,3],-1])\nT.append([[5,7],-1])\nperceptron.learning(la,T) # обучение перспетрона\nprint(perceptron.w) # печатаем веса\n# проверим работу на тестовых примерах\nprint(perceptron.sign([1.5, 2]))\nprint(perceptron.sign([3, 1.5]))\nprint(perceptron.sign([5, 1]))\nprint(perceptron.sign([5, 10]))","metadata":{"cell_id":"4e747925788147c89b140436f2cafbfb","source_hash":"809a027b","execution_start":1686039495460,"execution_millis":22,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[0.1, -0.1]\n-1\n1\n1\n-1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Видим, что что наш персептрон отлично научился распознавать образы, относя к классу 1 те вектора, у которых первая компонента больше второй, и к классу -1 в противном случае. Хотя устройство персептронов довольно простое эти конструкции могут решать и практические задачи. Кроме того, из таких персептронов состоят нейронные сети.","metadata":{"cell_id":"c6fb342894a84f56a1c14fc4e19052b5","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 1.1.2 Пример\nДля написания кода нейрона будем использовать библиотеку Pytnon — NumPy:","metadata":{"cell_id":"e2d8737a17644c7ea69036455268dae8","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import numpy as np\ndef sigmoid(x):\n    # Функция активации: f(x) = 1 / (1 + e*(-x))\n    return 1 / (1 + np.exp(-x))\nclass Neuron:\n    def __init__(self, weights, bias):\n        self.weights = weights\n        self.bias = bias\n    def feedforward(self, inputs):\n        total = np.dot(self.weights, inputs) + self.bias\n        return sigmoid(total)\n\nweights = np.array([0, 1])  # w1 = 0, w2 = 1\nbias = 4                    # с = 4\nn = Neuron(weights, bias) \nx = np.array([2, 3])        # x = 2, y = 3\nprint(n.feedforward(x))     # 0.9990889488055994","metadata":{"cell_id":"b8e78a0654aa4e349f6762ce048d6378","source_hash":"53b664a","execution_start":1686039495549,"execution_millis":33,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"0.9990889488055994\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Нейросеть состоит из множества соединенных между собой нейронов.\nПример несложной нейронной сети\n\n![img](https://i.ibb.co/0Cpmr9y/image.png)\n\nгде:\n- x1, x2 — входной слой;\n- h1, h2 — скрытый слой с двумя нейронами;\n- o1 — выходной слой.\nНапример. Представим, что нейроны из графика выше имеют веса [0, 1]. Пороговое значение (b) у обоих нейронов равно 0 и они имеют идентичную сигмоиду.\n\nПри входных данных x = [2, 3] получим:\n\n![img](https://i.ibb.co/m4wBZ4d/image.png)\n\nВходные данные по нейронам передаются до тех пор, пока не получатся выходные значения.","metadata":{"cell_id":"77e5a28a9f8c4875b1a384f022f5d701","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import numpy as np\nclass OurNeuralNetwork:\n    \n   \n    def __init__(self):\n        weights = np.array([0, 1])\n        bias = 0\n        # Knacc Neuron u3 предыдущего раздела\n        self.h1 = Neuron(weights, bias)\n        self.h2 = Neuron(weights, bias)\n        self.o1 = Neuron(weights, bias)\n    def feedforward(self, x):\n        out_h1 = self.h1.feedforward(x)\n        out_h2 = self.h2.feedforward(x)\n        # Входы для o1 — это входы h1 и h2\n        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n        return out_o1\n\nnetwork = OurNeuralNetwork()\nx = np.array([2, 3])\nprint(network.feedforward(x)) # 0.7216325609518421","metadata":{"cell_id":"1c356e58f5df4447930c559304bc01df","source_hash":"4d38967e","execution_start":1686039495648,"execution_millis":75,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"0.7216325609518421\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Задание (1.1.2)\nРеализовать классы нейросетей по аналогии с классом OurNeuralNetwork.\nДанные нейросети:\n- три входа (x1, x2, x3);\n- три нейрона в скрытых слоях (h1, h2, h3);\n- выход (o1)\n\nНейроны имеют идентичные веса и пороги:\n- w = [0.5, 0.5, 0.5]\n- b = 0\n\nДанные нейросети:\n- два входа (x1, x2);\n- два нейрона в скрытых слоях (h1, h2);\n- два выхода (o1, o2)\n\nНейроны имеют идентичные веса и пороги:\n- w = [1, 0];\n- b = 1","metadata":{"cell_id":"2ffbb936b3cb4340bb0a7f12b21599a3","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def sigmoid(x):\n    # Функция активации: f(x) = 1 / (1 + e*(-x))\n    return 1 / (1 + np.exp(-x))\nclass Neuron:\n    def __init__(self, weights, bias):\n        self.weights = weights\n        self.bias = bias\n    def feedforward(self, inputs):\n        total = np.dot(self.weights, inputs) + self.bias\n        return sigmoid(total)\n\nweights = np.array([0, 1])  # w1 = 0, w2 = 1\nbias = 4                    # с = 4\nn = Neuron(weights, bias) \nx = np.array([2, 3])        # x = 2, y = 3\nprint(n.feedforward(x))     # 0.9990889488055994","metadata":{"cell_id":"7894a000e84d445eb759581982054b23","source_hash":"80d5f41f","execution_start":1686039495769,"execution_millis":82,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"0.9990889488055994\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class OurNeuralNetwork:\n    def __init__(self):\n        weights = np.array([0.5, 0.5, 0.5]) # w = [1,0]\n        bias = 0 # b = 1\n        # Knacc Neuron u3 предыдущего раздела\n        self.h1 = Neuron(weights, bias) # 1 нейрон\n        self.h2 = Neuron(weights, bias) # 2 нейрон\n        self.h3 = Neuron(weights, bias) # 2 нейрон\n        self.o1 = Neuron(weights, bias) # 1 выход\n    def feedforward(self, x):\n        out_h1 = self.h1.feedforward(x)\n        out_h2 = self.h2.feedforward(x)\n        out_h3 = self.h2.feedforward(x)\n        # Входы для o1 — это входы h1 и h2\n        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n        return out_o1\n\nnetwork = OurNeuralNetwork()\nx = np.array([2, 3, 4])\nprint(network.feedforward(x)) # 0.7216325609518421","metadata":{"cell_id":"33847dd6f53146ee83f83ecfe93fe80a","source_hash":"5e4f4d3a","execution_start":1686039495924,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"0.8151036049051821\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class OurNeuralNetwork:\n    def __init__(self):\n        weights = np.array([1, 0]) # w = [1,0]\n        bias = 1 # b = 1\n        # Knacc Neuron u3 предыдущего раздела\n        self.h1 = Neuron(weights, bias) # 1 нейрон\n        self.h2 = Neuron(weights, bias) # 2 нейрон\n        self.o1 = Neuron(weights, bias) # 1 выход\n        self.o2 = Neuron(weights, bias) # 2 выход\n    def feedforward(self, x):\n        out_h1 = self.h1.feedforward(x)\n        out_h2 = self.h2.feedforward(x)\n        # Входы для o1 — это входы h1 и h2\n        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n        out_o2 = self.o2.feedforward(np.array([out_h1, out_h2]))\n        return out_o1, out_o2\n\nnetwork = OurNeuralNetwork()\nx = np.array([2, 3])\nprint(network.feedforward(x)) # 0.7216325609518421","metadata":{"cell_id":"0e3642662a974a3397611bdb8b89fd41","source_hash":"bc047e4b","execution_start":1686039495925,"execution_millis":50,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"(0.8757270529783324, 0.8757270529783324)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Задание\nРеализуйте классы нейронных сетей с использованием других функций активации.\n\n![img](https://i.ibb.co/Ln939nY/image.png)","metadata":{"cell_id":"f2ab55bc2f2544b5a544ddee34fc9461","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"## Sigmoid\ndef sigmoid(x):\n    sig = 1 / (1 + np.exp(-x))\n    return sig\n\nclass Neuron1:\n    def __init__(self, weights, bias):\n        self.weights = weights\n        self.bias = bias\n    def feedforward (self, inputs):\n        total = np.dot(self.weights, inputs) + self.bias\n        return sigmoid(total)\n\nclass OurNeuralNetwork:\n    def __init__(self):\n        weights = np.array([0.5, 0.5, 0.5])\n        bias = 0\n        self.h1 = Neuron1(weights, bias)\n        self.h2 = Neuron1(weights, bias)\n        self.h3 = Neuron1(weights, bias)\n        self.o1 = Neuron1(weights, bias)\n    def feedforward(self, x):\n        out_h1 = self.h1.feedforward(x)\n        out_h2 = self.h2.feedforward(x)\n        out_h3 = self.h3.feedforward(x)\n        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n        return out_o1\n\nclass OrNeuralNetwork:\n    def __init__(self):\n        weights = np.array([1,0])\n        bias = 1\n\n        self.h1 = Neuron1(weights, bias)\n        self.h2 = Neuron1(weights, bias)\n        self.o1 = Neuron1(weights, bias)\n        self.o2 = Neuron1(weights, bias)\n    def feedforward(self, x):\n        out_h1 = self.h1.feedforward(x)\n        out_h2 = self.h2.feedforward(x)\n        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n        out_o2 = self.o2.feedforward(np.array([out_h1, out_h2]))\n        return out_o1, out_o2\n\n\nnetwork = OurNeuralNetwork()\nx = np.array ([2, 3, 4])\nprint (network.feedforward(x))\n\nnetwork = OrNeuralNetwork()\nx = np.array ([2, 3])\nprint (network.feedforward(x))","metadata":{"cell_id":"ee2d333f5481472eb7b8202c61668202","source_hash":"9c59a9a4","execution_start":1686039495973,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"0.8151036049051821\n(0.8757270529783324, 0.8757270529783324)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"## Tanh\ndef tanh(x):\n    return np.tan(x)\n\n\nclass Neuron2:\n    def __init__(self, weights, bias):\n        self.weights = weights\n        self.bias = bias\n    def feedforward (self, inputs):\n        total = np.dot(self.weights, inputs) + self.bias\n        return tanh(total)\n\nclass OurNeuralNetwork:\n    def __init__(self):\n        weights = np.array([0.5, 0.5, 0.5])\n        bias = 0 \n        self.h1 = Neuron2(weights, bias)\n        self.h2 = Neuron2(weights, bias)\n        self.h3 = Neuron2(weights, bias)\n        self.o1 = Neuron2(weights, bias)\n    def feedforward(self, x):\n        out_h1 = self.h1.feedforward(x)\n        out_h2 = self.h2.feedforward(x)\n        out_h3 = self.h3.feedforward(x)\n        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n        return out_o1\n    \nnetwork = OurNeuralNetwork()\nx = np.array ([2, 3, 4])\nprint (network.feedforward(x))\n\nclass OurNeuralNetwork:\n    def __init__(self):\n        weights = np.array([1,0])\n        bias = 1\n        \n        self.h1 = Neuron2(weights, bias)\n        self.h2 = Neuron2(weights, bias)\n        self.o1 = Neuron2(weights, bias)\n        self.o2 = Neuron2(weights, bias)\n    def feedforward(self, x):\n        out_h1 = self.h1.feedforward(x)\n        out_h2 = self.h2.feedforward(x)\n        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n        out_o2 = self.o2.feedforward(np.array([out_h1, out_h2]))\n        return out_o1, out_o2\n    \nnetwork = OurNeuralNetwork()\nx = np.array ([2, 3])\nprint (network.feedforward(x))","metadata":{"cell_id":"32e8bbe1fe2f405889bbfa77aeeeaf9c","source_hash":"732c220b","execution_start":1686039495988,"execution_millis":21,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"0.7968426715486375\n(1.1555911185916798, 1.1555911185916798)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"## ReLU\ndef ReLU(x):\n    return np.maximum(0, x)\n\nclass Neuron3:\n    def __init__(self, weights, bias):\n        self.weights = weights\n        self.bias = bias\n    def feedforward (self, inputs):\n        total = np.dot(self.weights, inputs) + self.bias\n        return ReLU(total)\n\nclass OurNeuralNetwork:\n    def __init__(self):\n        weights = np.array([0.5, 0.5, 0.5])\n        bias = 0 \n        self.h1 = Neuron3(weights, bias)\n        self.h2 = Neuron3(weights, bias)\n        self.h3 = Neuron3(weights, bias)\n        self.o1 = Neuron3(weights, bias)\n    def feedforward(self, x):\n        out_h1 = self.h1.feedforward(x)\n        out_h2 = self.h2.feedforward(x)\n        out_h3 = self.h3.feedforward(x)\n        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n        return out_o1\n    \nnetwork = OurNeuralNetwork()\nx = np.array ([2, 3, 4])\nprint (network.feedforward(x))\n\nclass OurNeuralNetwork:\n    def __init__(self):\n        weights = np.array([1,0])\n        bias = 1\n        \n        self.h1 = Neuron3(weights, bias)\n        self.h2 = Neuron3(weights, bias)\n        self.o1 = Neuron3(weights, bias)\n        self.o2 = Neuron3(weights, bias)\n    def feedforward(self, x):\n        out_h1 = self.h1.feedforward(x)\n        out_h2 = self.h2.feedforward(x)\n        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n        out_o2 = self.o2.feedforward(np.array([out_h1, out_h2]))\n        return out_o1, out_o2\n    \nnetwork = OurNeuralNetwork()\nx = np.array ([2, 3])\nprint (network.feedforward(x))","metadata":{"cell_id":"e72c4f65407a494284d57ddac7ef7977","source_hash":"6020d016","execution_start":1686039496052,"execution_millis":47,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"6.75\n(4, 4)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"### Задание\nИспользуйте классы MLPClassified и MLPRegressor для классификации и регрессии произвольных данных из интернета. Проведите анализ атрибуты, полученных моделей.\n\nДля классификации можете взять набор данных Ирисов:\nhttps://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\n\nа для регрессии датасет зависимости заработной платы от опыта работы:\n\nhttps://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv","metadata":{"cell_id":"a05a80462c1d4d5db112c67ea304ff44","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"MLPClassifier — это клвсс, доступный как часть модуля neuro_network\r\nsklearn для выполнения задач классификации с использованием\r\nмногослойного персептрона.","metadata":{"cell_id":"6a1cc68c4910434da424eaaaf1405a97","formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"# MLPClassifier — это клвсс, доступный как часть модуля neuro_network\n# sklearn для выполнения задач классификации с использованием\n# многослойного персептрона.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nurl = 'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv'\ndf = pd.read_csv(url)\ndf.head(5)\ndf\ndf = df.rename(columns={'variety': 'target'})\nX_df, Y_df = df.drop(['target'], axis=1), df.target\nprint('Dataset Size: ', X_df.shape, Y_df.shape)\n# Функция train_test_split модуля model_selection sklearn поможет нам\n# разделить данные на два набора: 80% для обучения и 20% для тестирования.\n# Мы также используем seed(random_state=123) с train_test_split, чтобы мы\n# всегда получали одно и то же разделение и могли сравнивать и\n# воспроизволить результаты в будущем.\nX_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, train_size=0.80, test_size=0.20, stratify=Y_df, random_state=123)\nprint(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\nfrom sklearn.neural_network import MLPClassifier\n\n# Для начала натренируем модель MLPClassifier с параметрами по умолчанию\n# для тренировочных данных.\nmlp_classifier = MLPClassifier(random_state=123)\nmlp_classifier.fit(X_train, Y_train)\nY_preds = mlp_classifier.predict(X_test)\n\nprint(Y_preds[:15])\nprint(Y_test[:15])\n# Метод score для оценки точности моделей классификации\nprint('Test Accuracy: %.3f' % mlp_classifier.score(X_test, Y_test))\nprint('Training Accuracy: %.3f' % mlp_classifier.score(X_train, Y_train))\n\n# loss_ — возвращает убыток после завершения процесса обучения\nprint('Loss: ', mlp_classifier.loss_)\n# coefs_ — возвращает массив длины n_layers-1, где каждый элемент представляет веса, связанные с уровнем i\nprint('Number of Coefs: ', len(mlp_classifier.coefs_))\n# intercepts_ — возвращает массив длины n_layers-1, где каждый элемент представляет собой перехват, связанный с персептронами слоя i\nprint('Number of Intercepts: ', len(mlp_classifier.intercepts_))\n# n_iter_ — количество итераций, для которых выполнялась оценка\nprint('Number of Iteration for Which Estimator Ran: ', mlp_classifier.n_iter_)\n# out_activation_ — возвращает имя функции активации выходного слоя.\nprint('Name of Output Layer Activation Function: ', mlp_classifier.out_activation_)","metadata":{"cell_id":"d99438dadaff49be8f290258ea71ece0","source_hash":"10220a2c","execution_start":1686039496097,"execution_millis":2010,"is_output_hidden":false,"deepnote_to_be_reexecuted":false,"deepnote_app_is_output_hidden":true,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Dataset Size:  (150, 4) (150,)\n(120, 4) (30, 4) (120,) (30,)\n['Versicolor' 'Setosa' 'Virginica' 'Virginica' 'Setosa' 'Setosa'\n 'Virginica' 'Virginica' 'Virginica' 'Setosa' 'Setosa' 'Versicolor'\n 'Virginica' 'Versicolor' 'Virginica']\n80     Versicolor\n45         Setosa\n144     Virginica\n110     Virginica\n38         Setosa\n2          Setosa\n135     Virginica\n72     Versicolor\n138     Virginica\n34         Setosa\n19         Setosa\n77     Versicolor\n101     Virginica\n63     Versicolor\n117     Virginica\nName: target, dtype: object\nTest Accuracy: 0.933\nTraining Accuracy: 0.983\nLoss:  0.2988789340197433\nNumber of Coefs:  2\nNumber of Intercepts:  2\nNumber of Iteration for Which Estimator Ran:  200\nName of Output Layer Activation Function:  softmax\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"MLPRegressor — это класс, доступный как часть библиотеки\r\nneuro_network sklearn для выполнения задач регрессии с использованием\r\nмногослойного персептрона.","metadata":{"cell_id":"4f626ccebb61422e890e3cbc73b92d73","formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"url = 'https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv'\ndf = pd.read_csv(url)\ndf.head(5)\ndf\ndf = df.rename(columns={'Salary':'target'})\nX_df, Y_df = df.drop(['target'], axis=1), df.target\nprint ('Dataset Size: ', X_df.shape, Y_df.shape)\n# Функция train_test_split модуля model_selection sklearn поможет нам\n# разделить данные на два набора: 80% для обучения и 20% для тестирования.\n# Мы также используем seed(random_state=123) с train_test_split, чтобы мы\n# всегда получали одно и то же разделение и могли сравнивать и\n# воспроизволить результаты в будущем.\nX_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, train_size = 0.80, test_size = 0.20, random_state = 123)\nprint ('Train/Test size: ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\nfrom sklearn.neural_network import MLPRegressor\n\nmlp_regressor = MLPRegressor(random_state=123)\nmlp_regressor.fit(X_train, Y_train)\nY_preds = mlp_regressor.predict(X_test)\n\nprint (Y_preds[:10])\nprint (Y_test[:10])\nprint ('Test R^2 Score: %.3f'%mlp_regressor.score(X_test, Y_test))\nprint ('Training R^2 Score: %.3f'%mlp_regressor.score(X_train, Y_train))\n\nprint ('Loss: ', mlp_regressor.loss_)\nprint ('Number of Coefs: ', len(mlp_regressor.coefs_))\nprint ('Number of Intercepts: ', len(mlp_regressor.intercepts_))\nprint ('Number of Iteration for Which Estimator Ran: ', mlp_regressor.n_iter_)\nprint ('Name of Output Layer Activation Function: ', mlp_regressor.out_activation_)\n","metadata":{"cell_id":"7b542308a31047a19fcce31b5e413827","source_hash":"7cbe523e","execution_start":1686039498157,"execution_millis":224,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Dataset Size:  (30, 1) (30,)\nTrain/Test size:  (24, 1) (6, 1) (24,) (6,)\n[20.26234628 55.2781752  18.82135812 50.48274487 20.26234628 50.9622879 ]\n7      54445.0\n29    121872.0\n5      56642.0\n26    116969.0\n8      64445.0\n27    112635.0\nName: target, dtype: float64\nTest R^2 Score: -8.796\nTraining R^2 Score: -8.261\nLoss:  2988058032.1601596\nNumber of Coefs:  2\nNumber of Intercepts:  2\nNumber of Iteration for Which Estimator Ran:  200\nName of Output Layer Activation Function:  identity\n/shared-libs/python3.9/py/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=2480ccc6-6a1b-4590-b704-15c4e752c693' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_full_width":true,"deepnote_notebook_id":"af45eb4b309d4ffaa477ee3e1a982ff4","deepnote_execution_queue":[],"deepnote_persisted_session":{"createdAt":"2023-06-06T08:17:26.006Z"}}}