{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "### Рабочая тетрадь No 4",
      "metadata": {
        "cell_id": "3109cb0487484e5c95079325a3bdb50f",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1.1.1 Пример\nПроведем прямую y = mx + b через экспериментальные точки",
      "metadata": {
        "cell_id": "3907f516fda148c3a57d0cbcb65aec1f",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "19d9fc4d3e0644a6af6d17ce0ee05b16",
        "source_hash": "b7c6dccd",
        "execution_start": 1684829967545,
        "execution_millis": 932,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\n\nx = np.array([0, 1, 2, 3])\ny = np.array([-1, 0.2, 0.9, 2.1])\n\n# Перепишем линейное уравнение y = mx + C как y = Ap, где А = [[ x 1 ]] и p = [[m], [c]]\n# Построим A по Х:\n\nA = np.vstack([x,np.ones(len(x))]).T\nA # print забыли\n\n# Используем метод lstsq для решения его относительно вектора p\nm, c = np.linalg.lstsq(A, y, rcond = None)[0]\nprint(m, c)\n\n# Построим график полученной прямой и укажем на нем точки\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'o', label = 'исходные данные', markersize= 10)\n\nplt.plot(x, m*x + c, 'r',label = 'Линейная экстраполяция ')\nplt.legend()\nplt.show()",
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": "### 1.1.2 Пример\nПусть x, y – вектора длиной n > 3 (точек > 3). Задача заключается в построении эстраполяционного полинома второго порядка (параболы). Таким образом, необходимо найти такие коэффициенты поринома a, b, c по методу наименьших квадратов. Данные мтогут быть получены в результате измерений. Покажем пример генерации данных случайным образом и загрузки их из файла",
      "metadata": {
        "cell_id": "b359e3ea083243a1b6928484261535cb",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "299ae5ad029b475fab1dd03bec3f1af8",
        "source_hash": "aad7b42e",
        "execution_start": 1684829968520,
        "execution_millis": 229,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "from numpy import *\nfrom numpy.random import *\n\n# генерируем случайные x и y\ndelta = 1.0\nx = linspace(-5,5,11)\ny = x**2 + delta * (rand(11) - 0.5)\nx+= delta * (rand(11) - 0.5)\n\n# записывае данные в файл\nx.tofile('files/x_data.txt','\\n')\ny.tofile('files/y_data.txt','\\n')\n\n# читаем данные из файлов\nx = fromfile('files/x_data.txt', float, sep='\\n')\ny = fromfile('files/y_data.txt', float, sep='\\n')\n\n# Нахождение коэффициентов функции вида y = ax^2 + bx + c методом наименьших квадратов\n# задаем вектор m = [X**2, x, E]\nm = vstack((x**2,x,ones(11))).T\n\n# находим коэффициенты при составляющих вектора m\ns = np.linalg.lstsq(m,y,rcond = None)[0]\n\n# на отрезке [-5, 5]\nx_prec = linspace(-5, 5, 101)\n\n# рисуем точки\nplt.plot(x,y,'D')\n\n# рисуем кривую вида y = ax^2 + bx + c, подставляя из решения коэффициенты s[0], s[1], s[2]\nplt.plot(x_prec, s[0] * x_prec**2 + s[1] * x_prec+s[2], '-', lw = 2)\nplt.grid()\nplt.savefig(\"files/парабола.png\")",
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": "### 1.1.3 Пример\nПо данным предыдущего примера постройте эстраполяционного полинома третьего порядка",
      "metadata": {
        "cell_id": "c4a52583a53a498aac3bb7d1916018b2",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "63e32a8affb045bbb3567add894d9254",
        "source_hash": "a7e30061",
        "execution_start": 1684829968792,
        "execution_millis": 264,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "# Нахождение коэффициентов функции вида y = ax^3 + bx^2 + cx + d методом наименьших квадратов\n# задаем вектор m = [X**3, x, E]\nm = vstack((x**3, x**2, x, ones(11))).T\n\n# находим коэффициенты при составляющих вектора m\ns = np.linalg.lstsq(m, y, rcond = None)[0]\n\n# на отрезке [-5, 5]\nx_prec = linspace(-5, 5, 101)\n\n# рисуем точки\nplt.plot(x, y, 'D')\n\n# рисуем кривую вида y = ax^3 + bx^2 + cx + d, подставляя из решения коэффициенты s[0], s[1], s[2], s[3]\nplt.plot(x_prec, s[0] * x_prec**3 + s[1] * x_prec**2 + s[2]*x_prec + s[3], '-', lw = 3)\nplt.grid()\nplt.savefig('files/полином 3-й степени.png')",
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "### Задание (1.1.2 / 1.1.3)\nПредставьте собственные данные и постройте эктраполяцию полиномами первой, второй и третьей степени",
      "metadata": {
        "cell_id": "0e4b5b69dc364ceeb2152b8288ba95ad",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "3df6fffa247f485c8aa111efef98d35c",
        "source_hash": "3ba5dce7",
        "execution_start": 1684829969100,
        "execution_millis": 245,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "# генерируем случайные x и y\ndelta = 1.0\nx = linspace(-5,5,11)\ny = x**2 + delta * (rand(11) - 0.5)\nx+= delta * (rand(11) - 0.5)\n\n# записывае данные в файл\nx.tofile('files/x_data.txt', '\\n')\ny.tofile('files/y_data.txt', '\\n')\n\n# читаем данные из файлов\nx = fromfile('files/x_data.txt', float, sep='\\n')\ny = fromfile('files/y_data.txt', float, sep='\\n')\n\n# Нахождение коэффициентов функции вида y = ax^3 + bx^2 + cx + d методом наименьших квадратов\n# задаем вектор m = [X**3, x, E]\nm = vstack((x**3, x**2, x, ones(11))).T\n\n# находим коэффициенты при составляющих вектора m\ns = np.linalg.lstsq(m, y, rcond = None)[0]\n\n# на отрезке [-5, 5]\nx_prec = linspace(-5, 5, 101)\n\n# рисуем точки\nplt.plot(x,y,'D')\n\n# рисуем кривую вида y = ax^3 + bx^2 + cx + d, подставляя из решения коэффициенты s[0], s[1], s[2], s[3]\nplt.plot(x_prec, s[0] * x_prec**3 + s[1] * x_prec**2 + s[2]*x_prec + s[3], '-', lw = 3)\nplt.grid()\nplt.savefig(\"files/парабола.png\")",
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": "### 1.1.4 Пример\nНеобходимо проверить гипотезу, что наши точечно заданная функция ложится на кривую вида <img width=250px src=\"https://i.ibb.co/VJ7c9TQ/image.png\">",
      "metadata": {
        "cell_id": "665c22a241af426887c8ec74651bb8fd",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "0376866c08934cb7a3e64bc1297c8d61",
        "source_hash": "7fc89d74",
        "execution_start": 1684829969388,
        "execution_millis": 1035,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "import scipy as sp\n# Добавим шума в данные, сделанные по функции f(x,b) с коэффициентами b = (0.25, 0.75, 0.5)\nbeta = (0.25, 0.75, 0.5)\ndef f(x, b0, b1, b2):\n    return b0 + b1 * np.exp(-b2 * x**2)\n# зададим массив из точек xi\nxdata = np.linspace(0, 5, 50)\n# создаем теоретически правильные значения точек yi (без шума)\ny = f(xdata, *beta)\n# зашумляем эти данные\nydata = y + 0.05 * np.random.rand(len(xdata))\n# Используем функцию для получения решения в виде коэффициентов функции f(x) для указанных xdata и ydata\nfrom scipy.optimize import curve_fit\nbeta_opt, beta_cov = sp.optimize.curve_fit(f, xdata, ydata)\nbeta_opt\n# Вычислим линейное отклонение\nlin_dev = sum(beta_cov[0])\nlin_dev\n\n# Вычислим квадратичное уравнение\nresiduals = ydata - f(xdata, *beta_opt)\nfres = sum(residuals**2)\nfres\n\nfig, ax = plt.subplots()\nax.scatter(xdata,ydata)\nax.plot(xdata, y, 'r', lw=2)\nax.plot(xdata, f(xdata, *beta_opt), 'b', lw=2)\nax.set_xlim(0, 5)\nax.set_xlabel(r\"$x$\", fontsize = 18)\nax.set_ylabel(r\"$f(x,\\beta)$\", fontsize = 18)\nplt.show()\n",
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": "### 1.1.5 Пример\nНеобходимо проверить гипотезу, что наши точечно заданная функция ложится на кривые вида:\n\n<img width=250px src=\"https://i.ibb.co/SRZXNRt/image.png\">",
      "metadata": {
        "cell_id": "6f4995d0352f4382945c0444e401188f",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "90d6724df5d04525a6d45f2d51210024",
        "source_hash": "b7c4087",
        "execution_start": 1684829970421,
        "execution_millis": 480,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "#1\n# Добавим шума в данные, сделанные по функции f(x,b) с коэффициентами b = (0.25, 0.75)\nbeta = (0.25, 0.75)\ndef f(x, b0, b1):\n    return b0 + b1 * x\n# зададим массив из точек xi\nxdata = np.linspace(0, 5, 50)\n# создаем теоретически правильные значения точек yi (без шума)\ny = f(xdata, *beta)\n# зашумляем эти данные\nydata = y + 0.05 * np.random.rand(len(xdata))\nbeta_opt, beta_cov = sp.optimize.curve_fit(f, xdata, ydata)\nbeta_opt\n# Вычислим линейное отклонение\nlin_dev = sum(beta_cov[0])\nlin_dev\n\n# Вычислим квадратичное уравнение\nresiduals = ydata - f(xdata, *beta_opt)\nfres = sum(residuals**2)\nfres\n\nfig, ax = plt.subplots()\nax.scatter(xdata,ydata)\nax.plot(xdata, y, 'r', lw=2)\nax.plot(xdata, f(xdata, *beta_opt), 'b', lw=2)\nax.set_xlim(0, 5)\nax.set_xlabel(r\"$x$\", fontsize = 18)\nax.set_ylabel(r\"$f(x,\\beta)$\", fontsize = 18)\nplt.show()\n\n# Вычислим квадратичное уравнение\nresiduals = ydata - f(xdata, *beta_opt)\nfres = sum(residuals**2)\nfres\n\nfig, ax = plt.subplots()\nax.scatter(xdata,ydata)\nax.plot(xdata, y, 'r', lw=2)\nax.plot(xdata, f(xdata, *beta_opt), 'b', lw=2)\nax.set_xlim(0, 5)\nax.set_xlabel(r\"$x$\", fontsize = 18)\nax.set_ylabel(r\"$f(x,\\beta)$\", fontsize = 18)\nplt.show()",
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "769a5f978dd64e5ab4f7f29f4fb1655e",
        "source_hash": "7b0c8612",
        "execution_start": 1684829970914,
        "execution_millis": 292,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "#2\n# Добавим шума в данные, сделанные по функции f(x,b) с коэффициентами b = (0.25, 0.75, 0.5)\nbeta = (0.25, 0.75, 0.5)\ndef f(x, b0, b1, b2):\n    return b0 + b1 * x + b2 * x * x\n# зададим массив из точек xi\nxdata = np.linspace(0, 5, 50)\n# создаем теоретически правильные значения точек yi (без шума)\ny = f(xdata, *beta)\n# зашумляем эти данные\nydata = y + 0.05 * np.random.rand(len(xdata))\nbeta_opt, beta_cov = sp.optimize.curve_fit(f, xdata, ydata)\nbeta_opt\n# Вычислим линейное отклонение\nlin_dev = sum(beta_cov[0])\nlin_dev\n\n# Вычислим квадратичное уравнение\nresiduals = ydata - f(xdata, *beta_opt)\nfres = sum(residuals**2)\nfres\n\nfig, ax = plt.subplots()\nax.scatter(xdata,ydata)\nax.plot(xdata, y, 'r', lw=2)\nax.plot(xdata, f(xdata, *beta_opt), 'b', lw=2)\nax.set_xlim(0, 5)\nax.set_xlabel(r\"$x$\", fontsize = 18)\nax.set_ylabel(r\"$f(x,\\beta)$\", fontsize = 18)\nplt.show()",
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "a766dc41785c4148b3b549fa7761f43f",
        "source_hash": "62d7c42d",
        "execution_start": 1684829971219,
        "execution_millis": 287,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "#3\n# Добавим шума в данные, сделанные по функции f(x,b) с коэффициентами b = (1, 2)\nbeta = (1, 2)\ndef f(x, b0, b1):\n    return b0 + b1 * np.log(x)\n# зададим массив из точек xi\nxdata = np.linspace(1, 5, 50)\n# создаем теоретически правильные значения точек yi (без шума)\ny = f(xdata, *beta)\n# зашумляем эти данные\nydata = y + 0.05 * np.random.rand(len(xdata))\nbeta_opt, beta_cov = sp.optimize.curve_fit(f, xdata, ydata)\nbeta_opt\n# Вычислим линейное отклонение\nlin_dev = sum(beta_cov[0])\nlin_dev\n\n# Вычислим квадратичное уравнение\nresiduals = ydata -f(xdata, *beta_opt)\nfres = sum(residuals**2)\nfres\n\nfig,ax = plt.subplots()\nax.scatter(xdata,ydata)\nax.plot(xdata,y,'r',lw =2 )\nax.plot(xdata,f(xdata,*beta_opt),'b',lw = 2)\nax.set_xlim(0,5)\nax.set_xlabel(r\"$x$\", fontsize = 18)\nax.set_ylabel(r\"$f(x,\\beta)$\", fontsize = 18)\nplt.show()",
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "413a8be6005a4f77a672f2cf7b4de863",
        "source_hash": "3628921d",
        "execution_start": 1684829971505,
        "execution_millis": 257,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "#4\n# Добавим шума в данные, сделанные по функции f(x,b) с коэффициентами b = (1, 2)\nbeta = (1, 2)\ndef f(x, b0, b1):\n    return b0 * x ** b1\n# зададим массив из точек xi\nxdata = np.linspace(1, 5, 50)\n# создаем теоретически правильные значения точек yi (без шума)\ny = f(xdata, *beta)\n# зашумляем эти данные\nydata = y + 0.05 * np.random.rand(len(xdata))\nbeta_opt, beta_cov = sp.optimize.curve_fit(f, xdata, ydata)\nbeta_opt\n# Вычислим линейное отклонение\nlin_dev = sum(beta_cov[0])\nlin_dev\n\n# Вычислим квадратичное уравнение\nresiduals = ydata - f(xdata, *beta_opt)\nfres = sum(residuals**2)\nfres\n\nfig, ax = plt.subplots()\nax.scatter(xdata,ydata)\nax.plot(xdata, y, 'r', lw=2)\nax.plot(xdata, f(xdata, *beta_opt), 'b', lw=2)\nax.set_xlim(0, 5)\nax.set_xlabel(r\"$x$\", fontsize = 18)\nax.set_ylabel(r\"$f(x,\\beta)$\", fontsize = 18)\nplt.show()",
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": "### Задание (1.1.5)\nПодставьте собственные данные и поэкспериментируйте с представленными функциями. Проанализируйте динамику изменения данных.",
      "metadata": {
        "cell_id": "98a2e41e998a42bda072bd22279a59ae",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "ebd69c2ed24949febebf1a7e1e485b6e",
        "source_hash": "3939360d",
        "execution_start": 1684830136926,
        "execution_millis": 301,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "# 1\nbeta = (1, 0.1) # Добавим шума в данные, сделанные по функции f(x,b) с коэффициентами b\ndef f(x,b0,b1):\n    return b0 + b1*x\n# зададим массив из точек xi\nxdata = np.linspace(0,5,50)\n# создаем теоретически правильные значения точек yi (без шума)\ny = f(xdata, *beta)\n# зашумляем эти данные\nydata = y + 0.05 * np.random.rand(len(xdata))\nbeta_opt, beta_cov = sp.optimize.curve_fit(f,xdata,ydata)\nbeta_opt\n# Вычислим линейное отклонение\nlin_dev = sum(beta_cov[0])\nlin_dev\n\n# Вычислим квадратичное уравнение\nresiduals = ydata -f(xdata, *beta_opt)\nfres = sum(residuals**2)\nfres\n\nfig,ax = plt.subplots()\nax.scatter(xdata,ydata)\nax.plot(xdata,y,'r',lw =2 )\nax.plot(xdata,f(xdata,*beta_opt),'b',lw = 2)\nax.set_xlim(0,5)\nax.set_xlabel(r\"$x$\", fontsize = 18)\nax.set_ylabel(r\"$f(x,\\beta)$\", fontsize = 18)\nplt.show()",
      "outputs": [],
      "execution_count": 45
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "512e53bf7b134db98193bec493d8071b",
        "source_hash": "f5f1faa1",
        "execution_start": 1684830054548,
        "execution_millis": 252,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "# 2\nbeta = (5, 2, 1) # Добавим шума в данные, сделанные по функции f(x,b) с коэффициентами b\ndef f(x,b0,b1,b2):\n    return b0 + b1*x+b2*x*x\n# зададим массив из точек xi\nxdata = np.linspace(0,5,50)\n# создаем теоретически правильные значения точек yi (без шума)\ny = f(xdata, *beta)\n# зашумляем эти данные\nydata = y + 0.05 * np.random.rand(len(xdata))\nbeta_opt, beta_cov = sp.optimize.curve_fit(f,xdata,ydata)\nbeta_opt\n# Вычислим линейное отклонение\nlin_dev = sum(beta_cov[0])\nlin_dev\n\n# Вычислим квадратичное уравнение\nresiduals = ydata -f(xdata, *beta_opt)\nfres = sum(residuals**2)\nfres\n\nfig,ax = plt.subplots()\nax.scatter(xdata,ydata)\nax.plot(xdata,y,'r',lw =2 )\nax.plot(xdata,f(xdata,*beta_opt),'b',lw = 2)\nax.set_xlim(0,5)\nax.set_xlabel(r\"$x$\", fontsize = 18)\nax.set_ylabel(r\"$f(x,\\beta)$\", fontsize = 18)\nplt.show()",
      "outputs": [],
      "execution_count": 38
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "641dc49923c54298819c7c2beec728ce",
        "source_hash": "724f6fd6",
        "execution_start": 1684830089730,
        "execution_millis": 403,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "# 3\nbeta = (2, 4) # Добавим шума в данные, сделанные по функции f(x,b) с коэффициентами b\ndef f(x,b0,b1):\n    return b0 + b1*np.log(x)\n# зададим массив из точек xi\nxdata = np.linspace(3,10,70)\n# создаем теоретически правильные значения точек yi (без шума)\ny = f(xdata, *beta)\n# зашумляем эти данные\nydata = y + 0.05 * np.random.rand(len(xdata))\nbeta_opt, beta_cov = sp.optimize.curve_fit(f,xdata,ydata)\nbeta_opt\n# Вычислим линейное отклонение\nlin_dev = sum(beta_cov[0])\nlin_dev\n\n# Вычислим квадратичное уравнение\nresiduals = ydata -f(xdata, *beta_opt)\nfres = sum(residuals**2)\nfres\n\nfig,ax = plt.subplots()\nax.scatter(xdata,ydata)\nax.plot(xdata,y,'r',lw =2 )\nax.plot(xdata,f(xdata,*beta_opt),'b',lw = 2)\nax.set_xlim(0,5)\nax.set_xlabel(r\"$x$\", fontsize = 18)\nax.set_ylabel(r\"$f(x,\\beta)$\", fontsize = 18)\nplt.show()",
      "outputs": [],
      "execution_count": 43
    },
    {
      "cell_type": "markdown",
      "source": "### 1.2.1 Пример\nПостроим простую линейную регрессию в Python с использованием библиотеки scikit-learn",
      "metadata": {
        "cell_id": "2faa92ec3ba341c0bbc2690f05054fbe",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "После того как мы получили представление о данных, разделим информацию на «атрибуты» и «метки». Атрибуты – это независимые переменные, а метки – это зависимые переменные, значения которых должны быть предсказаны. В нашем наборе всего два столбца и необходимо предсказать оценку в зависимости от количества часов. Чтобы извлечь атрибуты и метки, выполните следующий скрипт:",
      "metadata": {
        "cell_id": "4e4572daa881497eb1fd9f2d22d30da9",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "0f99bbd3e333478d9ab7de3e8742e0f4",
        "source_hash": "f42f5469",
        "execution_start": 1684829972498,
        "execution_millis": 691,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "# Импортируем необходимые библиотеки\n# используем pandas и numpy для обработки данных,\n# matplotlib для визуализации и sklearn для обучения наборов данных и импорта моделей\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame, Series\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression",
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "f2987febf3014b6abc3ef37c9bf4b85a",
        "source_hash": "c9b7c415",
        "execution_start": 1684829973202,
        "execution_millis": 247,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "# создадим набор данных для описания взаимосвязи между временем обучения студентов и успеваеимостью\nmy_dict = {'Учебное время':[0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50],\n        \"Оценка\":[10,22,13,43,20,22,33,50,62,48,55,75,62,73,81,76,64,82,90,93]}\n\n\n\ndataset = pd.DataFrame(my_dict)\ndataset.head()\n\n# Исследуем набор данных\nprint(dataset.shape)\ndataset.describe()\n\n# Нарисуем точечную диаграмму\nplt.scatter(dataset[\"Учебное время\"],dataset['Оценка'],color ='b',label = \"данные экзамена\")\nplt.xlabel(\"Часы\")\nplt.ylabel(\"Оценка\")\nplt.show()",
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "288dd1cd800147c2b921f25bf94970d2",
        "source_hash": "9f17694",
        "execution_start": 1684829973448,
        "execution_millis": 4,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "x = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,1].values\nprint(x)\nprint(y)\n\n# Теперь когда у нас есть атрибуты и метки, необходимо разделить их на a обучающий и тестовый наборы\n# Приведенный фрагмент разделяет 80% данных на обучающий набор, а 20% данных - на набор тестов\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size = 0.2, random_state = 0)\n\n# далее можно обучить алгоритм линейной регрессии\n# необходимо импортировать класс LinearRegression, создать его экземляр и вызвать метод fit()\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# приведем получившиеся коэффициенты для линии регрессии\nprint(regressor.intercept_)\nprint(regressor.coef_)",
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": "Получившийся результат можно интерпретировать следующим образом: с каждым затраченным часом на обучение результат экзамена повышается приблизительно на 17 баллов. Далее можно построить прогнозы. Для этого мы будем использовать наши тестовые данные и посмотрим, насколько точно наш алгоритм предсказывает процентную оценку. Чтобы сделать прогноз на тестовых данных необходимо выполнить следующий код:",
      "metadata": {
        "cell_id": "daedc09dda314da28cf9dfec41665700",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "65c5439b0f144c27a9b902218cc45cb8",
        "source_hash": "f9a67877",
        "execution_start": 1684829973457,
        "execution_millis": 633,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "y_pred = regressor.predict(X_test)\n# сравним фактические значения с прогнозируемыми\ndf = pd.DataFrame({'Actual': y_test, \"Predicted\": y_pred})\n\n# визуазилируем результат сравнения в виде гистограммы\ndf.plot(kind = 'bar')\nplt.grid(which='major', linestyle = '-', linewidth ='0.5', color = 'green')\nplt.grid(which='minor', linestyle = ':', linewidth ='0.5', color = 'black')\nplt.show()\n\n# построим линию регрессии с тестовыми данными\nplt.scatter(X_test, y_test, color = 'gray')\nplt.plot(X_test, y_pred, color = 'red', linewidth=2)\nplt.show()",
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": "### Задание (1.2.1)\nПостройте модель линейной регрессии для произвольных данных из двух столбцов. Для примера можно взять точечную зависимость заработной платы от опыта работы:\n\n(https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv).\n\nНайдите коэффициенты линии регрессии. Постройте прогноз.",
      "metadata": {
        "cell_id": "278f39a89b4b4e80bed4363f0664a67f",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "50723a0cb43e4765b4c8f8dbaaa89f3f",
        "source_hash": "3938a7df",
        "is_code_hidden": false,
        "execution_start": 1684829974094,
        "execution_millis": 352,
        "is_output_hidden": false,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "url = \"https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv\"\ndataset = pd.read_csv(url)\n\n# Исследуем набор данных\nprint(dataset.shape)\ndataset.describe()\n\n# Нарисуем точечную диаграмму\nplt.scatter(dataset['YearsExperience'],dataset[\"Salary\"],color = 'r',label = \"Salary data \")\nplt.xlabel(\"YearsExperience\")\nplt.ylabel(\"Salary\")\nplt.show()",
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "6e281c6d99da4aa9a8397137d02eacbd",
        "source_hash": "9f17694",
        "execution_start": 1684829974397,
        "execution_millis": 50,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "x = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,1].values\nprint(x)\nprint(y)\n\n# Теперь когда у нас есть атрибуты и метки, необходимо разделить их на a обучающий и тестовый наборы\n# Приведенный фрагмент разделяет 80% данных на обучающий набор, а 20% данных - на набор тестов\nX_train, X_test, y_train, y_test = train_test_split(x, y, train_size = 0.2, random_state = 0)\n\n# далее можно обучить алгоритм линейной регрессии\n# необходимо импортировать класс LinearRegression, создать его экземляр и вызвать метод fit()\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# приведем получившиеся коэффициенты для линии регрессии\nprint(regressor.intercept_)\nprint(regressor.coef_)",
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "b182e82a9cf74086869b5c9575c10402",
        "source_hash": "125b775d",
        "execution_start": 1684829974402,
        "execution_millis": 690,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "y_pred = regressor.predict(X_test)\n# сравним фактические значения с прогнозируемыми\ndf = pd.DataFrame({'Actual': y_test, \"Predicted\": y_pred})\n\n\n# визуазилируем результат сравнения в виде гистограммы\ndf.plot(kind = 'bar')\nplt.grid(which='major', linestyle = '-', linewidth ='0.5', color = 'green')\nplt.grid(which='minor', linestyle = ':', linewidth ='0.5', color = 'black')\nplt.show()\n\n# построим линию регрессии с тестовыми данными\nplt.scatter(X_test,y_test,color = 'gray')\nplt.plot(X_test,y_pred,color = 'red', linewidth=2)\nplt.show()",
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": "### 1.3.1 Пример\nДля решения задачи множественной регрессии можно задействовать уже известный метод numpy.linalg.lstsq",
      "metadata": {
        "cell_id": "50a75862d72644c6b91590cf207a0c7c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "f7cef340eaa94e46a4caf7ff6158b67a",
        "source_hash": "53814acf",
        "execution_start": 1684830449748,
        "execution_millis": 2,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "y = [1,2,3,4,3,4,5,3,5,5,4,5,4,5,4,5,6,0,6,3,1,3,1]\nX = [[0,2,4,1,5,4,5,9,9,9,3,7,8,8,6,6,5,5,5,6,6,5,5],\n     [4,1,2,3,4,5,6,7,5,8,7,8,7,8,7,8,6,8,9,2,1,5,6],\n     [4,1,2,5,6,7,8,9,7,8,7,8,4,4,3,1,2,3,4,1,3,9,7]]\n\nX = np.transpose(X)\nX = np.c_[X,np.ones(X.shape[0])]\nlinreg = np.linalg.lstsq(X,y,rcond=None)[0]\nprint(linreg)",
      "outputs": [],
      "execution_count": 46
    },
    {
      "cell_type": "markdown",
      "source": "Кроме этого можно использовать возможности библиотеки sсikit-learn. Рассмотрим пример.",
      "metadata": {
        "cell_id": "9ae926c10e7448ff820e4d1f35f66333",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1.3.2 Пример\nДля данных из предыдущей задачи построить модель множественной линейной регрессии с использованием средств библиотеки sсikit-learn",
      "metadata": {
        "cell_id": "5e1ef5f6914242798c3e20b697053340",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "da67c008ef92488583b0038d6a1eac6e",
        "source_hash": "120dd1cf",
        "execution_start": 1684830535904,
        "execution_millis": 2,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "from  sklearn import metrics\n\ny = [1,2,3,4,3,4,5,3,5,5,4,5,4,5,4,5,6,0,6,3,1,3,1]\nX = [[0,2,4,1,5,4,5,9,9,9,3,7,8,8,6,6,5,5,5,6,6,5,5],\n     [4,1,2,3,4,5,6,7,5,8,7,8,7,8,7,8,6,8,9,2,1,5,6],\n     [4,1,2,5,6,7,8,9,7,8,7,8,4,4,3,1,2,3,4,1,3,9,7]]\n\n# формируем DataFrame из двух списков\nnew_y = np.array(y)\nnew_y = new_y.transpose()\ndf1 = pd.DataFrame(new_y)\nnew_X = np.array(X)\nnew_X = new_X.transpose()\ndf2 = pd.DataFrame(new_X)\ndf1 = df1.rename(columns={0:'y'}, inplace= False)\ndf2 = df2.rename(columns={0:'x1',1:'x2',2:'x3'},inplace= False)\n\nframes = [df1,df2]\ndataset = pd.concat([df1,df2], axis=1,join=\"inner\")\nprint(dataset.head()) # принт\n\n# изучим данные\nprint(dataset.shape)\ndataset.describe()\n\n# разделим данные на метки и атрибуты\nX = dataset[['x1','x2','x3']]\ny = dataset['y']\n \n# разделим данные на обучающую и тестовую выборки\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.2,random_state=0)\n\n# для обучения алгоритма мы выполняем тот же код, что и раньше, используя метод fit() класса LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)\n\n# выведем коэфициент модели\ncoeff_df =  pd.DataFrame(regressor.coef_,X.columns, columns=[\"Coefficient\"])\nprint(coeff_df) # принт\n\n# чтобы сделать прогнозы на тестовых данных, выполните следующй код\ny_pred = regressor.predict(X_test)\ndf = pd.DataFrame({\"Actual\":y_test,'Predicted':y_pred})\nprint(df) # принт\n\n# последний шаг - оценить производительность алгоритма. Мы сделаем это, найдя все значения для MSE \nprint(\"Mean Squared Error:\", metrics.mean_squared_error(y_test,y_pred))",
      "outputs": [],
      "execution_count": 49
    },
    {
      "cell_type": "markdown",
      "source": "### Задание (1.2.1)\nПостройте модель множественной линейной регрессии для произвольных данных из нескольких столбцов. Для примера можно взять потребления газа (в миллионах галлонов) в 48 штатах США или набор данных о качестве красного вина (1) и (2) соответственно. Найдите коэффициенты множественной регрессии. Постройте прогноз\n\n1. https://raw.githubusercontent.com/likarajo/petrol_consumption/master/data/petrol_consumption.csv\n\n2. https://raw.githubusercontent.com/aniruddhachoudhury/Red-Wine-Quality/master/winequality-red.csv",
      "metadata": {
        "cell_id": "447e86a31e3649379c88e7aafd3a18b6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "a1375d3c4f0d44fe99558f44e5946e71",
        "source_hash": "ccc0f233",
        "execution_start": 1684829975162,
        "execution_millis": 180,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "url1 = 'https://raw.githubusercontent.com/likarajo/petrol_consumption/master/data/petrol_consumption.csv'\nurl2 = 'https://raw.githubusercontent.com/aniruddhachoudhury/Red-Wine-Quality/master/winequality-red.csv'\n\ndataset1 = pd.read_csv(url1)\ndataset2 = pd.read_csv(url2)",
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "ae26c50df6b44bbeb21bf8eeffcc6686",
        "source_hash": "d6328d73",
        "execution_start": 1684829975347,
        "execution_millis": 6,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "x = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,1].values\n\n# Приведенный фрагмент разделяет 80% данных на обучающий набор, а 20% данных - на набор тестов\nX_train1, X_test1, y_train1, y_test1 = train_test_split(x, y, test_size = 0.2, random_state = 0)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\n# далее можно обучить алгоритм линейной регрессии\n# необходимо импортировать класс LinearRegression, создать его экземляр и вызвать метод fit()\nregressor1 = LinearRegression()\nregressor1.fit(X_train1, y_train1)\n\nregressor2 = LinearRegression()\nregressor2.fit(X_train2, y_train2)",
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "ed575e7cc5e04093bff1e5b86ff73067",
        "source_hash": "c5997a1",
        "execution_start": 1684829975360,
        "execution_millis": 103,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "y_pred1 = regressor1.predict(X_test1)\n# сравним фактические значения с прогнозируемыми\ndf1 = pd.DataFrame({'Actual': y_test1, 'Predicted': y_pred1})\ndisplay(df1)\n\ny_pred2 = regressor2.predict(X_test2)\ndf2 = pd.DataFrame({'Actual': y_test2, 'Predicted': y_pred2})\ndisplay(df2)\n\nprint('Mean Squared Error first csv: ', metrics.mean_squared_error(y_test1, y_pred1))\nprint('Mean Squared Error second csv: ', metrics.mean_squared_error(y_test2, y_pred2))",
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "source": "### Задание*\nЗадача: Экспериментально получены N − значений величины Y при различных значениях величины X. Построить полиномы первой и второй степени, аппроксимирующие результаты эксперимента, с применением метода наименьших квадратов. Результаты выводятся в виде таблиц значений и графиков, полученных полиномов\n\n![Picture title](https://i.ibb.co/TWDr4K2/image.png)\n",
      "metadata": {
        "cell_id": "64ec7da272144554a6db1d122106b973",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "fa925b7673044ef7a0a8350711aa5039",
        "source_hash": "672acbd5",
        "execution_start": 1684832133508,
        "execution_millis": 716,
        "deepnote_to_be_reexecuted": false,
        "deepnote_cell_type": "code"
      },
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.array([4.0, 4.2, 4.4, 4.6, 4.8, 5.0])\ny = np.array([4.0, 3.0, 6.0, 6.0, 4.0, 4.0])\n\nA = np.vstack([x, np.ones(len(x))]).T\nprint(A)\n\nm, c = np.linalg.lstsq(A, y, rcond = None)[0]\nprint(m, c)\n\nplt.plot(x, y, 'o', label = 'Исходные данные', markersize = 10)\nplt.plot(x, m*x + c, 'r', label = 'Линейная экстраполяция')\nplt.legend()\nplt.show()\n\nm = np.vstack((x ** 2, x, np.ones(6))).T\n\ns = np.linalg.lstsq(m, y, rcond = None)[0]\n\nx_prec = np.linspace(4, 5, 6)\n\nplt.plot(x, y, 'D')\nplt.plot(x_prec, s[0] * x_prec ** 2 + s[1] * x_prec + s[2], '-', lw = 2)\nplt.grid()\nprint(s)",
      "outputs": [],
      "execution_count": 50
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=2480ccc6-6a1b-4590-b704-15c4e752c693' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "nbconvert_exporter": "python"
    },
    "orig_nbformat": 4,
    "deepnote_full_width": true,
    "deepnote_notebook_id": "7cb10474858c4fa082e7d777eb36f75f",
    "deepnote_persisted_session": {
      "createdAt": "2023-05-23T09:31:26.232Z"
    },
    "deepnote_execution_queue": []
  }
}